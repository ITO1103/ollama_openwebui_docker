version: '3.4'

# ///////////   LINUX ONLY   ////////////
# Steps To Run LINUX with gpu Acceleration (need ROOT access like = sudo docker compose up)
# 1. Install Docker and NVIDIA Container Toolkit
#  https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
# sudo apt-get install -y nvidia-container-toolkit
# sudo nvidia-ctk runtime configure --runtime=docker
# sudo systemctl restart docker


# test if GPU acceleration is available
#  firs RUN:  docker pull nvidia/cuda:12.8.1-runtime-ubuntu24.04
#  next To test if GPU acceleration is available, run the following command:

#  For cuda 12.8 use this command:
#  sudo docker run --rm --gpus all   nvidia/cuda:12.8.1-runtime-ubuntu24.04 nvidia-smi

# For cuda 12.2 use this command:
#  sudo docker run --rm --gpus all   nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi

#  FINALLY RUN: sudo docker compose up


# COMUN ERROR:
# When your pc is on sleep mode or hibernation mode, the GPU will not be available for docker containers.
# 1. Check if the GPU is available by running the following command:
#  nvidia-smi
# 2. If the GPU is still not available, restart the nvidia_uvm module by running the following command:
#  sudo rmmod nvidia_uvm && sudo modprobe nvidia_uvm
# 3 Restart Your PC :D



# //////////////////////////////////////////////////////////////////

services:
    nginx:
        image: nginx:alpine
        restart: unless-stopped
        ports:
            - '8333:80'
        volumes:
            - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro # Nginx設定ファイルをマウント
        depends_on:
            - openwebui # openwebuiが起動してからnginxを起動
    openwebui:
        image: ghcr.io/open-webui/open-webui:0.6.5
        restart: unless-stopped
        depends_on:
            - ollamadeepseek
        # ports:
        #    - '8333:8080'
        volumes:
          - './open-webui:/app/backend/data'
        environment:
            - OLLAMA_BASE_URL=http://ollamadeepseek:11434 # ホストIPではなくサービス名を使用
            - OLLAMA_HOST=http://ollamadeepseek:11434 # Ollamaホストも明示的に設定
            #- HOST=0.0.0.0
    ollamadeepseek: 
        build:
            context: .
            dockerfile: Dockerfile
        restart: unless-stopped
        ports:
           - '11434:11434'
        volumes:
          - './ollama:/root/.ollama'
        environment:
            OLLAMA_HOST: 0.0.0.0
            OLLAMA_PORT: 11434
            OLLAMA_MODELS: 
            # CUDA_VISIBLE_DEVICES: 0, 1
            # OLLAMA_MAX_LOADED_MODELS: 3
            # OLLAMA_NUM_PARALLEL: 2
        #  To use only CPU, Just  comment the following line or remove it
        deploy:
          resources:
            reservations:
              devices:
                - driver: nvidia
                  count: all
                  capabilities: [gpu]
